import { w } from './../utils/utils.js';

export default {
  title: `Google's problem with AI and ethics`,
  type: 'podcast',
  id: '538e56jgjyioijou',
  thumbnail: '538e5y0jy1kmha3',
  header: `Google's ${w('problem', 0)} with AI and ${w('ethics', 1)}`,
  author: { name: 'Jordan Erica Webber', url: 'https://www.theguardian.com/profile/jordan-erica-webber' },
  link: { name: 'Chips with everything - The Guardian', url: 'https://www.theguardian.com/technology/audio/2019/may/06/googles-problem-with-ai-and-ethics-chips-with-everything-podcast' },
  source: 'https://flex.acast.com/audio.guim.co.uk/2019/05/03-52177-gnl.chips.05052019.ds.ai_ethic.mp3',
  subtitles: new Map([
    ['00:01', `The Guardian`],
    ['00:06', `So, I ${w('think', 2)} it's ${w('fair', 3)} to be ${w('worried', 4)} about AI, ${w('you know', 5)}, so, I ${w("wouldn't say", 7)} they're ${w('just', 8)} ${w('being', 9)} ${w('optimistic', 10)} about it`],
    ['00:12', `But we ${w('want', 11)} to be ${w('thoughtful', 12)} about it. AI ${w('holds', 13)} the ${w('potential', 14)} for some of the ${w('biggest', 15)} ${w('advances', 16)} we are going to ${w('see', 17)}.`],
    ['00:19', `${w('At the end of', 18)} ${w('March', 19)} Google ${w('launched', 20)} a ${w('global', 21)} ${w('council', 22)} that would ${w('advise', 23)} on ${w('ethical', 24)} ${w('issues', 25)} ${w('around', 26)}`],
    ['00:25', `${w('artificial intelligence', 27)} and ${w('other', 28)} ${w('emerging', 29)} ${w('technologies', 30)}. They ${w('called', 31)} it The ${w('Advanced', 2000)} ${w('Technology', 2001)} ${w('External', 2002)} ${w('Advisory', 2003)} ${w('Council', 2004)}.`],
    ['00:33', `A ${w('week', 33)} ${w('later', 34)}`],
    ['00:35', `Google ${w('is under fire', 35)} ${w('again', 36)} for ${w('pulling the plug on', 37)} its AI ${w('Ethics', 38)} ${w('Board', 39)}. They ${w('announced', 40)} they were ${w('shutting', 41)} it ${w('down', 41)}.`],
    ['00:43', `A ${w('group', 42)} of Google ${w('employees', 43)} had ${w('protested', 44)} the ${w('inclusion', 45)} of what they ${w('considered', 46)} to be a ${w('right-wing', 47)} ${w('think-tank', 49)} ${w('leader', 50)}`],
    ['00:50', `and had ${w('called for', 51)} her ${w('removal', 52)} ${w('because of', 53)} ${w('previous', 54)} ${w('remarks', 55)} she had made that were ${w('thought', 56)} to be ${w('anti', 57)}-${w('LGBT', 2005)} and ${w('anti', 58)}-${w('immigrant', 59)}.`],
    ['00:58', `${w('Another', 60)} ${w('member', 61)} of the ${w('council', 62)} had ${w('already', 63)} ${w('resigned', 64)}.`],
    ['01:01', `In a ${w('statement', 65)} Google ${w('said', 66)}: "it's ${w('become', 67)} ${w('clear', 68)} that in the ${w('current', 69)} ${w('environment', 70)}`],
    ['01:05', `ATEAC can't ${w('function', 71)} as we ${w('wanted', 72)}. So we're ${w('ending', 73)} the ${w('council', 74)} and ${w('going back to the drawing board', 75)}.`],
    ['01:13', `This ${w('got', 76)} me ${w('thinking of', 2006)} an ${w('episode', 77)} we did in the ${w('summer', 78)} of 2018`],
    ['01:17', `when Google's CEO Sundar Pichai ${w('wrote', 79)} a ${w('blog', 80)} ${w('post', 81)} ${w('outlining', 82)} ${w('seven', 83)} ${w('objectives', 84)} the ${w('company', 85)} had ${w('going forward', 86)} for ${w('working', 87)} with AI`],
    ['01:27', `At Chips we're ${w('constantly', 88)} ${w('looking at', 89)} how ${w('big tech', 90)} is ${w('attempting to', 91)} ${w('grapple with', 92)} the ${w('ethics', 93)} of the AI they're ${w('making', 94)}.`],
    ['01:33', `So this ${w('week', 95)} we're going to ${w('replay', 96)} that ${w('episode', 97)} to ${w('refresh', 98)} our ${w('memories', 99)} about what Google had ${w('hoped', 100)} to ${w('achieve', 101)} when they ${w('first', 102)} ${w('set out', 103)} these ${w('principles', 104)}.`],
    ['01:42', `Don't ${w('worry', 105)}. We'll ${w('be back', 106)} ${w('next', 2007)} ${w(' week', 107)} with a new ${w('episode', 108)} of Chips. I'm Jordan Erica Webber and this is Chips With Everything.`],
    ['01:58', `This week we've ${w('brought together', 110)} some of the best ${w('minds', 111)} ${w('across', 112)} ${w('tech', 113)} and ${w('ethics', 114)} to ${w('discuss', 115)} ${w('ethical', 116)} ${w('standards', 117)} for the ${w('research', 118)} and ${w('development', 119)} of AI.`],
    ['02:08', `So this ${w('week', 120)} I'm ${w('joined', 121)} in ${w('studio', 122)} by Sanjay Modgil, a ${w('senior', 2008)} ${w('lecturer', 123)} from the ${w('Reasoning', 124)} and ${w('Planning', 125)} ${w('Group', 126)} at King's ${w('College', 127)}, London`],
    ['02:15', `Dr. Yasmin Arden a ${w('senior', 128)} ${w('lecturer', 129)} of ${w('philosophy', 130)} at Samara University in London. And on skype we're ${w('joined', 131)} by`],
    ['02:22', `dr Sandra Wachter, ${w('research fellow', 132)} in ${w('data', 134)} ${w('ethics', 135)}, AI, ${w('robotics', 137)} and ${w('internet', 138)} ${w('regulation', 139)}`],
    ['02:28', `${w('Cyber security', 140)} at The Oxford Internet ${w('Institute', 141)} and The Alan Turing ${w('Institute', 142)} in London`],
    ['02:33', `So that's ${w('three', 143)} ${w('people', 144)} who are ${w('incredibly', 145)} ${w('qualified', 146)} to ${w('help', 147)} us ${w('discuss', 148)} this ${w('week', 149)}'s ${w('topic', 150)}`],
    ['02:37', `I should also ${w('point out', 151)} that we did ${w('reach out', 152)} to DeepMind to see if ${w('someone', 153)} from the ${w('company', 154)} ${w('wanted', 155)} to ${w('discuss', 156)} this with us,`],
    ['02:43', `but they weren't ${w('available', 157)} ${w('at this time', 158)}.`],
    ['02:46', `${w('So', 159)}, Google's CEO Sundar Pichai`],
    ['02:49', `or ${w('at least', 160)} ${w('someone', 161)} from his ${w('office', 162)} ${w('published', 163)} a ${w('blog', 164)} ${w('post', 165)} in ${w('early', 167)} ${w('June', 168)} that ${w('outlined', 169)} Google's ${w('seven', 170)} ${w('principles', 171)} for ${w('working', 172)} with AI`],
    ['02:57', `${w('plus', 174)} ${w('four', 175)} ${w('areas', 176)} in ${w('which', 177)} they will not ${w('use', 178)} AI. You've ${w('all', 180)} ${w('had a look at', 181)} them and we'll ${w('get into', 182)} these ${w('specific', 183)}`],
    ['03:03', `${w('commandments', 184)} that ${w('intrigue', 185)} you ${w('in particular', 186)} ${w('later on', 187)}, but first`],
    ['03:06', `Sandra, just from you, could we get some context for the conversation? So just, could you remind us of Google's history with AI?`],
    ['03:14', `Yes, so I think`],
    ['03:15', `Google in particular but actually all major tech companies have become very aware of the ethical implications`],
    ['03:23', `of AI and for quite some time`],
    ['03:25', `I think everybody was very keen on finding a way forward to at one hand`],
    ['03:31', `harness the full potential of AI but at the same time find solutions to mitigate those risks. And I think`],
    ['03:38', `the statement now is a very good step forward to actually pledge that some ethical consideration will be`],
    ['03:45', `at the heart of their work.`],
    ['03:48', `I'm interested in why Google is doing this now. Have they got into trouble before for the area research? Anyone?`],
    ['03:55', `Well`],
    ['03:55', `It does seem as if the staff rather rejected their work on military drones`],
    ['04:01', `and I think that that puts enormous pressure on them to do something. - Is that Project Maven? - That's right.`],
    ['04:07', `I think also there has been some talk recently about the fact that there.. "Don't do evil"`],
    ['04:12', `Some sort of principle has been quietly dropped from their commitments.`],
    ['04:17', `"Don't be evil" had long been Google's unofficial company motto`],
    ['04:20', `but when the company was reorganized under its parent company Alphabet, it was slightly adjusted to "Do the right thing",`],
    ['04:28', `So Yasmine`],
    ['04:29', `It's not the first time that rules or principles have been written up by organizations that are concerned with using area responsibly.`],
    ['04:36', `In fact, the Future of Life Institute, for example, has had`],
    ['04:39', `hundreds of AI researchers sign its list of AI principles including many Google employees.`],
    ['04:44', `So, why did Sundar Pichai decide to write his own list and why right now?`],
    ['04:48', `I mean, it seems like a very good step forward`],
    ['04:50', `I can't help but be slightly cynical and it does seem to me rather more of a PR sort of offensive than, than much else.`],
    ['04:58', `For instance, I'm quite concerned`],
    ['05:00', `They're called objectives, which doesn't suggest to me an intention to make this,`],
    ['05:04', `you know, something concrete, that you must abide by in the same way that something like rules or laws, or principles as you described would.. would suggest`],
    ['05:12', `Yeah, I think they do come across as rather vague`],
    ['05:14', `and of course the devil is always in the detail, but I do think it's a step in the right direction`],
    ['05:20', `Let's hope that there are some follow-up work on these principles. This have made more concrete that there does`],
    ['05:26', `prompt a discussion as to what the appropriate regulations are`],
    ['05:29', `And what I think we really have to see now how these.. how these principles are now fleshed out in a bit more detail`],
    ['05:34', `the very vagueness of the principles means that you who might follow them and`],
    ['05:39', `inadvertently that may still lead to harmful consequences. I mean in some sense. This is the rhetorical thrust`],
    ['05:44', `ironically of Asimov's Three Laws. If we encode these kinds of principles in the AI themselves`],
    ['05:51', `they may still eventually cause harm even though they abide by those principles in their.. in a general sense.`],
    ['05:58', `Before we started recording I asked each of my guests to pick one of these seven objectives that stood out to them as particularly interesting.`],
    ['06:06', `So we're gonna dig into some of these principles in more detail, and we'll start with you, Sanjay.`],
    ['06:10', `Of these seven commandments or objectives as they're called, laid out by Pichai, which one in particular intrigued you?`],
    ['06:17', `I started with the one on`],
    ['06:19', `on being built and tested for safety. So of course, there's a major issue and a major concerns about the safety of artificial intelligence.`],
    ['06:27', `What interested me was`],
    ['06:30', `many of these doomsday scenarios have anticipated`],
    ['06:33', `problems arising when these artificially intelligent technologies are developed by the companies.`],
    ['06:38', `So, what strikes me`],
    ['06:40', `a difficult issue would be when the machines.. these artificial intelligent machines start to be used by a Google DeepMind.`],
    ['06:46', `They start to become more autonomous, they start to engage in research and development themselves, and`],
    ['06:51', `then we might have the problem that the actions of these machines will not be aligned with human values.`],
    ['06:57', `So that's.. that goal is a very complex goal to unpack and to make sure that it is.. it is actually satisfied.`],
    ['07:04', `So that's principle number three - be built and tested for safety - and I'll just elaborate on what Pichai writes there`],
    ['07:09', `He says "We will continue to develop and apply strong, safety and security practices to avoid unintended results that create risks of harm.`],
    ['07:16', `We will design our AI systems to be appropriately cautious and seek to develop them in accordance with best practices in AI safety research.`],
    ['07:24', `In appropriate cases, we will test AI technologies in constrained environments and monitor their operation after deployment".`],
    ['07:30', `Yasmine, it seemed like you had something you wanted to say there.`],
    ['07:32', `Well, I think one of the problems there is again in this terminology that they're adopting.`],
    ['07:36', `They talk about being appropriately cautious`],
    ['07:38', `and I mean that.. that leaves so much room for ambiguity and`],
    ['07:43', `what we think about as appropriate and in what context and according to what needs and interests.`],
    ['07:49', `Also to be cautious doesn't mean that they don't do something you can still do something and be cautious about it at the same time,`],
    ['07:56', `so it doesn't really commit them to all of that much`],
    ['07:59', `And I have the same sort of concerns with a lot of these`],
    ['08:03', `objectives, but when it comes to safety and security that's.. that's really a very big problem.`],
    ['08:09', `Yeah, does anyone have any thoughts for how you actually find out for AI is safe?`],
    ['08:13', `Maybe it's not so much a question of finding out how an AI is safe but`],
    ['08:16', `ensuring that an AI is safe in the sense that..`],
    ['08:20', `in order to make an appropriate ethical decision one would require`],
    ['08:24', `input from the humans. I quite like the idea that they had`],
    ['08:28', `I mean, I know it is very very very general, but they have principle for be accountable to people.`],
    ['08:33', `Now, I think the suggestion there that's very much linked to this issue`],
    ['08:36', `which is that these ethical decisions will need to have input from the humans on an ongoing basis`],
    ['08:41', `because of course our.. our moral ethical systems evolve over time.`],
    ['08:46', `It's not like you can prescribe something from the outset and then let it go and assume it will be appropriate.`],
    ['08:51', `Does anyone have any thoughts on whether this will affect things like autonomous vehicles?`],
    ['08:55', `So the idea that something has to be tested for safety if it has the potential to kill people.`],
    ['09:00', `Does that mean you just shouldn't work on it at all? - I think there is.. it's very important to distinguish is`],
    ['09:07', `research versus deployment, right?`],
    ['09:09', `I think those things are very often mixed together and that's not actually accurate.`],
    ['09:14', `You can research and test a lot of things before you make the decision to deploy it. So, when it comes to critical`],
    ['09:21', `safety systems, like autonomous cars, I don't think anybody would argue that we shouldn't be testing them.`],
    ['09:28', `In fact, everybody would agree that we should test it in a lot of different and diverse environments to make them learn`],
    ['09:35', `from their mistakes and stuff like that.`],
    ['09:37', `The other question is when is a product safe enough to be released to the market and I think this is where we probably..`],
    ['09:45', `this is where the regulation stuff starts, not so much with the research actually.`],
    ['09:50', `Do you think that an outside body should regulate the safety of AI at Google?`],
    ['09:54', `I don't think there is like a one fits all solutions to that`],
    ['09:58', `I guess there are algorithms that have not so many risks attached to them`],
    ['10:02', `we might not need oversights, but for certain areas, I think it might be irresponsible to not have oversight.`],
    ['10:09', `I mean, of course, we have`],
    ['10:12', `public bodies regulating how to use cars and we need to get certificates for being allowed to use them on the street and`],
    ['10:19', `because there's a risk attached to it,`],
    ['10:21', `so if there is a deployment or a system that has similar issues, then yes,`],
    ['10:25', `of course, we need oversight from a governmental or independent body.`],
    ['10:29', `Yasmine, you mentioned Project Maven.`],
    ['10:31', `If Google was following this principle of making sure things are tested for safety,`],
    ['10:35', `do you think they never would have been able to pick up that contract in the first place?`],
    ['10:38', `Well, I think in a way they've sort of left space to do that because they've said that, you know,`],
    ['10:43', `they're not going to be involved with military equipment`],
    ['10:47', `per se, but they did say they would still be working with governments`],
    ['10:50', `and they gave a list of, you know, ways in which that might play out.`],
    ['10:54', `So again, I'm not sure that any of these objectives or present a particular barrier to that sort of work.`],
    ['11:03', `What's an interesting thing would be, well, what if Google were involved in let's say the`],
    ['11:08', `development of a certain kind of sensor technology that was extremely accurate at identifying an ISIS terrorist.`],
    ['11:15', `Now, that sensor technology could be used`],
    ['11:18', `by an autonomous`],
    ['11:20', `weapon, which was not developed by`],
    ['11:22', `Google, but it still`],
    ['11:25', `makes crucial use of a technology that's been developed by Google.`],
    ['11:29', `Now, maybe this principle allows and gives them the leeway to develop such a sensor technology`],
    ['11:37', `and one furthermore could argue that on utilitarian grounds that - actually on balance - that would be a good thing.`],
    ['11:42', `What actually would it be detecting when you say a terrorist? - I mean, it's a thought experiment if you like.`],
    ['11:47', `I'm a philosopher, so I love thought experiments, but I would also say that there was a real problem with them.`],
    ['11:52', `I think that they sometimes seduce us into thinking that a scenario that could in principle, at least in our minds,`],
    ['11:58', `be real, therefore has some possibility in reality. And the risk with that is if things happen as a result of that possibility.`],
    ['12:05', `So I think we need to be really careful about either suggesting or AI could do`],
    ['12:12', `in a way that actually is not going to happen or`],
    ['12:16', `suggesting what it could do and therefore we need to take account of that in terms of what we`],
    ['12:20', `think about ethically and in legal terms.`],
    ['12:23', `So, I'm not really sure how our technology resolves the problem`],
    ['12:28', `which makes`],
    ['12:30', `good use or a bad use, if you prefer, of the sorts of problems that we have in our`],
    ['12:36', `human engagement and then feeds them into technology.`],
    ['12:42', `So it's difficult to envisage exactly how Google will make sure the AIs they deploy are fully safe.`],
    ['12:48', `But a plan is in place and the Sandra explains - The blog post is symbolic. It represents the beginning of an important conversation.`],
    ['13:00', `After the break we'll discuss Google's ongoing issue with unfair biases in their AI software and what it means to be`],
    ['13:07', `accountable to people when working with AIs. - About to this point from a legal perspective.`],
    ['13:14', `There wasn't much that you could actually do because we really don't have any laws governing AI.`],
    ['13:24', `We'll be right back.`],
    ['13:32', `The voice slap from The Guardian.`],
    ['13:35', `Hey, do you have a one a quick catch up on the news headlines first thing in the morning while you're making breakfast or getting dressed?`],
    ['13:43', `Well, if you have a Google Assistant or Google Home, we can help with that.`],
    ['13:47', `The Guardian Briefing is an experiment from the Voice Lab, which in under two ${w('minutes', 1000)}`],
    ['13:53', `${w('brings', 1001)} you ${w('up to speed', 1001)} with what you ${w('need', 1003)} to ${w('know', 1004)} about the ${w('day', 1005)}'s ${w('top', 1006)} ${w('stories', 1007)}.`],
    ['13:58', `We'll ${w('make sure', 1008)} you don't ${w('miss a thing', 1009)}. To ${w('listen', 1010)} ${w('at any time', 1011)} ${w('just', 1012)} ${w('say', 1013)} - Hey Google, ${w('speak to', 1014)} the Guardian Briefing.`],
    ['14:09', `${w('Welcome back', 1015)} to Chips With Everything. I'm Jordan Erica Webber.`],
    ['14:15', `${w('Before', 1016)} the ${w('break', 1017)} I and a ${w('small', 1018)} ${w('group', 1019)} of ${w('experts', 1020)} started to dissect a recent blog post from Google CEO Sundar Pichai.`],
    ['14:23', `Which outlines the technology companies seven objectives for responsibly working with AIs.`],
    ['14:30', `It seems pretty obvious the AI can reinforce bias in some pretty horrific ways.`],
    ['14:34', `Just this week my friend Chella who actually co-hosted this podcast for our Black History Month episode`],
    ['14:39', `tweeted that Samsung had created a storyline from her photos called my furry friend that included photos`],
    ['14:46', `she'd taken of animals and one picture of her with her hair out.`],
    ['14:50', `Now, Google has also made some serious mistakes on this kind of thing,`],
    ['14:53', `but, do you have any ideas how Samsung, Google and everyone else can actually avoid reinforcing unfair bias?`],
    ['14:59', `I'm actually not at all surprised by that.`],
    ['15:01', `I'm horrified by it, but I'm not still surprised and the reason I'm not is because of the scandal in January of this year when`],
    ['15:08', `Google facial recognition - image recognition -`],
    ['15:12', `identified human beings as gorillas, and it was... their response to that, that particularly struck me, because what they did very quickly`],
    ['15:19', `was to remove the use of the term gorilla in that image recognition software.`],
    ['15:26', `Now, that doesn't get to the problem of what that software is doing and the biases that it's replicating.`],
    ['15:30', `I think that for as long as they keep`],
    ['15:33', `looking for these post hoc solutions that don't really tackle why the`],
    ['15:37', `problems end up in the software, then it's just going to repeat itself. - What do you think Sanjay? Do you think it's possible to`],
    ['15:43', `avoid bias with this kind of tech? - It's going to be very difficult`],
    ['15:46', `I mean we've already seen as.. we've seen many examples of bias and in some sense the data, of course,`],
    ['15:52', `includes the bias and the predominant paradigm in artificial intelligence')} is machine learning which is essentially learning from`],
    ['16:00', `precedent, from the data, and I think this is behind principle four, which is`],
    ['16:05', `"Corrections of these biases will require that the machines explain their reasoning and make their reasoning transparent" and`],
    ['16:13', `my understanding and the hope is that they are aware that it's a problem and they are working on solutions`],
    ['16:20', `albeit, they may.. they may not have developed particularly sophisticated solutions and in particular the problem, of course, is with explaining the reasoning, but`],
    ['16:28', `That's my understanding of how they're looking to address this issue.`],
    ['16:35', `What strikes me that.. it is phrased as we should, you know, minimize biases.`],
    ['16:41', `I think it would be smart to say - we should increase diversity - and I think that would be actually`],
    ['16:48', `helping the problem, because that the problems of bias and discrimination`],
    ['16:52', `is partly because the data is biased and it just replicates the stereotypes that we have.`],
    ['16:58', `We also have a problem in the coding community in terms of biases because, you know, decoding community is not very diverse.`],
    ['17:06', `So obviously those people have`],
    ['17:09', `intentional and unintentional and blatant biases. They are just gonna, you know..`],
    ['17:13', `Everybody has certain convictions and this will be fed into the algorithm as well. And the last thing is`],
    ['17:20', `diversity in terms of disciplines. There is probably not just a tech solution to the problems that we're facing.`],
    ['17:26', `We actually need different disciplines to`],
    ['17:28', `think about this together. When you talk about the political impacts of AI, political scientists are important.`],
    ['17:34', `Are you talking about the economic impacts of AI? You probably need economists for that. Is it about the legal implications?`],
    ['17:40', `We need lawyers. It is about the ethical implications? You need philosophers.`],
    ['17:44', `So it's not just about tweaking the algorithm to make it, you know, seem unbiased.`],
    ['17:49', `It's actually a holistic project that you need to embrace and that's diversity.`],
    ['17:55', `I mean perhaps again`],
    ['17:57', `this will be a place where regulation could dictate that there is the kind of for example the required diversity in the bias.`],
    ['18:03', `Correctors, you've got this. You know, a team of bias correctors that say - no, no, no, this is bias and..`],
    ['18:09', `regulation could help there to say - okay, we need to make sure you've got a full, you know, representation of different cultures and`],
    ['18:15', `genders, etc, etc. And that's where regulation could really make a difference.`],
    ['18:18', `So, our panel agrees that when it comes to AI is creating or reinforcing unfair bias, we can't solve the problem with more technology.`],
    ['18:26', `We need to look to human beings.`],
    ['18:29', `But what happens when issues do arise?`],
    ['18:32', `Who is held accountable? And how?`],
    ['18:35', `Which brings us to Sandra's chosen objective.`],
    ['18:38', `So the issue of accountability has come up quite a lot already in this discussion`],
    ['18:42', `but Sandra you wanted to dig a little deeper into this, which is principle number four,`],
    ['18:46', `which is "Be accountable to people", so Pichai writes - "we will design AI systems that provide appropriate opportunities for feedback`],
    ['18:53', `relevant explanations and appeal.`],
    ['18:55', `Our AI technologies will be subject to appropriate human direction and control". So up until this point how has this kind of thing worked if people had`],
    ['19:03', `complaints about AI,`],
    ['19:05', `how did Google deal with them?`],
    ['19:07', `So far there has been a very vivid discussion about accountability in general because, you know,`],
    ['19:12', `the algorithms are very unpredictable, and what they do, so you don't really foresee`],
    ['19:18', `how are they gonna make decisions and what they're gonna do? If they do it`],
    ['19:22', `you might not be able to understand it and they work very anonymously.`],
    ['19:26', `So it's basically the algorithm`],
    ['19:29', `doing stuff and not necessarily human and all those things are very often used to say -`],
    ['19:34', `well, you know, it's not the humans fault,`],
    ['19:37', `the algorithm did it. And that's a very`],
    ['19:39', `bad, bad excuse and I think an excuse that we should not accept in a society in the whole.`],
    ['19:44', `- Computer said no, and I don't know how it works. But let's just move on,`],
    ['19:50', `let's not try to open up the black box - is a very big problem. Up until this point from a legal perspective`],
    ['19:56', `there wasn't much that you could actually do because we really don't have any laws governing AI at all.`],
    ['20:04', `And we do now have`],
    ['20:06', `the new data protection framework in Europe. That's a very good step forward in ensuring explainable and`],
    ['20:12', `transparent AI and giving users or the general public a way to`],
    ['20:16', `start engaging in dialogue rather than saying - This is a black box. We don't understand it.`],
    ['20:21', `And that is the end of the discussion.`],
    ['20:23', `And in fact, if you were to ask someone who was a research for machine learning, well`],
    ['20:28', `can you explain,`],
    ['20:30', `why your machine learning algorithm made the decision it did? They'd be flummoxed, frankly.`],
    ['20:35', `So, it's one of the most pressing problems. Yeah, it's still quite woolly. It offers a very promising`],
    ['20:43', `intention, but yes, I certainly can't see what that concrete outcome could look like.`],
    ['20:49', `Especially when you take into account quite how many different problems may arise and do arise in response to these sorts of AI systems.`],
    ['20:58', `Sandra, do you think that this goal would be better achieved if the tech industry as a whole just kind of came together and called for`],
    ['21:03', `a regulatory body to specifically look at companies that create AIs?`],
    ['21:08', `Yes, I think.. even though, I think like the first step forward was very good of Google to say what they think`],
    ['21:17', `how the industry should move forward.`],
    ['21:18', `I think the next step is actually to start a conversation with the whole of industry,`],
    ['21:23', `for example The Partnership on AI would be a very good place to have that and I'm very much hope that this will actually`],
    ['21:29', `start a fruitful conversation not just with the tech industry, but also with civil society, and academics and government around that.`],
    ['21:39', `I'm not saying you should regulate everything, but to a certain extent if we have real problems, then yes`],
    ['21:46', `we probably need some regulatory oversight for those things.`],
    ['21:55', `That's all for today. Come back to us next week for a brand new episode.`],
    ['22:00', `Chips is produced by Danielle Stevens. I'm Jordan Erica Webber. Thanks for listening.`],
    ['22:14', `For more great podcasts from the Guardian just go to the guardian.com/podcasts`]
  ]),
  words: [
    { index: 0, id: '538e53wsjvmtvefm', meaning: [0, 1] },
    { index: 1, id: '538e53wsjvmtvefn', meaning: [0, 1] },
    { index: 2, id: '538e55asjtwvhdne', meaning: [0, 2] },
    { index: 3, id: '538e53wsjvmtvefo', meaning: [0] },
    { index: 4, id: '538e53wsjvmtvefp', meaning: [1] },
    { index: 5, id: '538e53wsjvmtvefq', meaning: [0, 1] },
    { index: 7, id: '538e53wsjvmtvefr', meaning: [4, 3] },
    { index: 8, id: '538e54tojtwvi2k3', meaning: [1, 2] },
    { index: 9, id: '538e53wsjvmtvefs', meaning: [0] },
    { index: 10, id: '538e53wsjvmtveft', meaning: [0] },
    { index: 11, id: '538e53wsjvmtvefu', meaning: [0] },
    { index: 12, id: '538e53wsjvmtvefv', meaning: [0] },
    { index: 13, id: '538e53wsjvmtvefw', meaning: [0, 1] },
    { index: 14, id: '538e53wsjvmtvefx', meaning: [0] },
    { index: 15, id: '538e53wsjvmtvefy', meaning: [0, 1] },
    { index: 16, id: '538e53wsjvmtvefz', meaning: [0] },
    { index: 17, id: '538e53wsjvmtveg0', meaning: [1, 2] },
    { index: 18, id: '538e53wsjvmtveg1', meaning: [0, 1] },
    { index: 19, id: '538e53wsjvmtveg2', meaning: [0] },
    { index: 20, id: '538e53wsjvmtveg3', meaning: [0, 1] },
    { index: 21, id: '538e53wsjvmtvf1z', meaning: [0] },
    { index: 22, id: '538e53wsjvmtvf20', meaning: [0] },
    { index: 23, id: '538e53wsjvmtvf21', meaning: [0, 1] },
    { index: 24, id: '538e53wsjvmtvf22', meaning: [0, 1] },
    { index: 25, id: '538e53wsjvmtvf23', meaning: [0] },
    { index: 26, id: '538e53wsjvmtvf24', meaning: [0, 1] },
    { index: 27, id: '538e53wsjvmtvf25', meaning: [0] },
    { index: 28, id: '538e53wsjvmtvepa', meaning: [0] },
    { index: 29, id: '538e53wsjvmtvf26', meaning: [0, 1, 3, 2] },
    { index: 30, id: '538e53wsjvmtvf0h', meaning: [0] },
    { index: 31, id: '538e53wsjvmtvf0i', meaning: [0] },
    { index: 2000, id: '538e53wsjvmtvf27', meaning: [0, 1] },
    { index: 2001, id: '538e53wsjvmtvf0h', meaning: [0] },
    { index: 2002, id: '538e53wsjvmtvf28', meaning: [1, 0] },
    { index: 2003, id: '538e53wsjvmtvf29', meaning: [0] },
    { index: 2004, id: '538e53wsjvmtvf20', meaning: [0] },
    { index: 33, id: '538e53wsjvmtvf2a', meaning: [0] },
    { index: 34, id: '538e53wsjvmtvf2b', meaning: [0] },
    { index: 35, id: '538e53wsjvmtvf2c', meaning: [0, 1] },
    { index: 36, id: '538e53wsjvmtvf2d', meaning: [0, 1, 2] },
    { index: 37, id: '538e53wsjvmtvf2e', meaning: [0, 1, 2] },
    { index: 38, id: '538e53wsjvmtvefn', meaning: [0, 1] },
    { index: 39, id: '538e53wsjvmtvf2f', meaning: [0, 1] },
    { index: 40, id: '538e53wsjvmtvf0q', meaning: [0, 1, 2] },
    { index: 41, id: '538e53wsjvmtvf2g', meaning: [0] },
    { index: 42, id: '538e54j4jtwvjlfx', meaning: [0] },
    { index: 43, id: '538e53wsjvmtvf2h', meaning: [0] },
    { index: 44, id: '538e53wsjvmtvf2i', meaning: [1, 0] },
    { index: 45, id: '538e53wsjvmtvf2j', meaning: [0, 1] },
    { index: 46, id: '538e53wsjvmtvf2k', meaning: [0, 1] },
    { index: 47, id: '538e53wsjvmtvf2l', meaning: [0] },
    { index: 49, id: '538e53wsjvmtvf2m', meaning: [0] },
    { index: 50, id: '538e53wsjvmtvf2n', meaning: [1, 3] },
    { index: 51, id: '538e53wsjvmtvf2o', meaning: [0] },
    { index: 52, id: '538e53wsjvmtvf2p', meaning: [0, 1] },
    { index: 53, id: '538e53wsjvmtveoi', meaning: [0, 1] },
    { index: 54, id: '538e53wsjvmtvf2q', meaning: [0, 1] },
    { index: 55, id: '538e53wsjvmtvf2r', meaning: [0] },
    { index: 56, id: '538e54igjtwvh8jv', meaning: [3] },
    { index: 57, id: '538e53wsjvmtvf2s', meaning: [0, 1] },
    { index: 2005, id: '538e53wsjvmtvf2t', meaning: [0] },
    { index: 58, id: '538e53wsjvmtvf2s', meaning: [0, 1] },
    { index: 59, id: '538e53wsjvmtvf2u', meaning: [0] },
    { index: 60, id: '538e53wsjvmtvf2v', meaning: [0, 1] },
    { index: 61, id: '538e53wsjvmtvf2w', meaning: [0] },
    { index: 62, id: '538e53wsjvmtvf20', meaning: [0] },
    { index: 63, id: '538e53wsjvmtvef4', meaning: [0] },
    { index: 64, id: '538e53wsjvmtvf2x', meaning: [1, 0] },
    { index: 65, id: '538e53wsjvmtvf2y', meaning: [0] },
    { index: 66, id: '538e53wsjvmtvebl', meaning: [0] },
    { index: 67, id: '538e53wsjvmtvevu', meaning: [0] },
    { index: 68, id: '538e53wsjvmtvf2z', meaning: [0, 2] },
    { index: 69, id: '538e53wsjvmtvf30', meaning: [0] },
    { index: 70, id: '538e53wsjvmtveci', meaning: [2] },
    { index: 71, id: '538e53wsjvmtvf31', meaning: [0, 1] },
    { index: 72, id: '538e53wsjvmtvefu', meaning: [0] },
    { index: 73, id: '538e53wsjvmtvf32', meaning: [0, 1] },
    { index: 74, id: '538e53wsjvmtvf20', meaning: [0] },
    { index: 75, id: '538e53wsjvmtvf33', meaning: [1, 0] },
    { index: 76, id: '538e51pgk1m7rjzt', meaning: [1] },
    { index: 2006, id: '538e51pgk1m7rjzu', meaning: [0] },
    { index: 77, id: '538e53wsjvmtveyq', meaning: [0] },
    { index: 78, id: '538e51pgk1m7rjzv', meaning: [0] },
    { index: 79, id: '538e51pgk1m7rjzw', meaning: [0] },
    { index: 80, id: '538e51pgk1m7rjzx', meaning: [0] },
    { index: 81, id: '538e51pgk1m7rjzy', meaning: [0, 1] },
    { index: 82, id: '538e51pgk1m7rjzz', meaning: [0, 1] },
    { index: 83, id: '538e51pgk1m7rk00', meaning: [0] },
    { index: 84, id: '538e51pgk1m7rk01', meaning: [0] },
    { index: 85, id: '538e51pgk1m7rk02', meaning: [1, 0] },
    { index: 86, id: '538e51pgk1m7rk03', meaning: [0, 2] },
    { index: 87, id: '538e53wsjvmtvf0y', meaning: [0] },
    { index: 88, id: '538e51pgk1m7rk04', meaning: [0, 1] },
    { index: 89, id: '538e51pgk1m7rk05', meaning: [0] },
    { index: 90, id: '538e51pgk1m7rk06', meaning: [0] },
    { index: 91, id: '538e51pgk1m7rk07', meaning: [0, 1] },
    { index: 92, id: '538e53wsjvmtvez2', meaning: [0] },
    { index: 93, id: '538e53wsjvmtvefn', meaning: [0] },
    { index: 94, id: '538e53wsjvmtveew', meaning: [1] },
    { index: 95, id: '538e53wsjvmtvf2a', meaning: [0] },
    { index: 96, id: '538e51pgk1m7rk08', meaning: [0, 1] },
    { index: 97, id: '538e53wsjvmtveyq', meaning: [0] },
    { index: 98, id: '538e51pgk1m7rk09', meaning: [0] },
    { index: 99, id: '538e53wsjvmtven5', meaning: [1] },
    { index: 100, id: '538e51pgk1m7rk0a', meaning: [0] },
    { index: 101, id: '538e51pgk1m7rk0b', meaning: [0, 2] },
    { index: 102, id: '538e51pgk1m7rk0c', meaning: [0, 1] },
    { index: 103, id: '538e51pgk1m7rk0d', meaning: [0, 1] },
    { index: 104, id: '538e51pgk1m7rk0e', meaning: [0, 1] },
    { index: 105, id: '538e51pgk1m7rk0f', meaning: [0] },
    { index: 106, id: '538e51pgk1m7rk0g', meaning: [0, 1] },
    { index: 2007, id: '538e51pgk1m7rk0h', meaning: [0, 1, 2] },
    { index: 107, id: '538e53wsjvmtvf2a', meaning: [0] },
    { index: 108, id: '538e53wsjvmtveyq', meaning: [0] },
    { index: 110, id: '538e51pgk1m7rk0i', meaning: [0] },
    { index: 111, id: '538e51pgk1m7rk0j', meaning: [0] },
    { index: 112, id: '538e51pgk1m7rk0k', meaning: [2, 1] },
    { index: 113, id: '538e53wsjvmtvf0h', meaning: [0] },
    { index: 114, id: '538e53wsjvmtvefn', meaning: [0, 1] },
    { index: 115, id: '538e51pgk1m7rk0l', meaning: [0, 1, 2] },
    { index: 116, id: '538e53wsjvmtvf22', meaning: [0, 1] },
    { index: 117, id: '538e51pgk1m7rk0m', meaning: [0] },
    { index: 118, id: '538e5998jtwvfsbw', meaning: [0] },
    { index: 119, id: '538e51pgk1m7rk0n', meaning: [0, 1] },
    { index: 120, id: '538e53wsjvmtvf2a', meaning: [0] },
    { index: 121, id: '538e51pgk1m7rk0o', meaning: [3, 2] },
    { index: 122, id: '538e51pgk1m7rk0p', meaning: [0] },
    { index: 2008, id: '538e51pgk1m7rk0q', meaning: [0] },
    { index: 123, id: '538e51pgk1m7rk0r', meaning: [0] },
    { index: 124, id: '538e51pgk1m7rk0s', meaning: [0] },
    { index: 125, id: '538e51pgk1m7rk0t', meaning: [0] },
    { index: 126, id: '538e54j4jtwvjlfx', meaning: [0] },
    { index: 127, id: '538e51pgk1m7rk0u', meaning: [0] },
    { index: 128, id: '538e51pgk1m7rk0q', meaning: [0] },
    { index: 129, id: '538e51pgk1m7rk0r', meaning: [0] },
    { index: 130, id: '538e51pgk1m7rk0v', meaning: [0] },
    { index: 131, id: '538e51pgk1m7rk0o', meaning: [3, 2] },
    { index: 132, id: '538e51pgk1m7rk0w', meaning: [0] },
    { index: 134, id: '538e51pgk1m7rk0x', meaning: [0] },
    { index: 135, id: '538e53wsjvmtvefn', meaning: [0, 1] },
    { index: 137, id: '538e51pgk1m7rk0y', meaning: [0] },
    { index: 138, id: '538e51pgk1m7rk0z', meaning: [0] },
    { index: 139, id: '538e51pgk1m7rk11', meaning: [0] },
    { index: 140, id: '538e51pgk1m7rk10', meaning: [0] },
    { index: 141, id: '538e51pgk1m7rk12', meaning: [0] },
    { index: 142, id: '538e51pgk1m7rk12', meaning: [0] },
    { index: 143, id: '538e53wsjvmtvezk', meaning: [0] },
    { index: 144, id: '538e58aojtwdgbp2', meaning: [1] },
    { index: 145, id: '538e53wsjvmtvf0j', meaning: [1, 2] },
    { index: 146, id: '538e51pgk1m7rk13', meaning: [0, 1] },
    { index: 147, id: '538e51pgk1m7rk14', meaning: [0] },
    { index: 148, id: '538e51pgk1m7rk0l', meaning: [0, 1, 2] },
    { index: 149, id: '538e53wsjvmtvf2a', meaning: [0] },
    { index: 150, id: '538e53wsjvmtvex9', meaning: [0] },
    { index: 151, id: '538e51pgk1m7rk15', meaning: [0, 3] },
    { index: 152, id: '538e51pgk1m7rk16', meaning: [2, 1] },
    { index: 153, id: '538e51pgk1m7rk17', meaning: [0] },
    { index: 154, id: '538e51pgk1m7rk02', meaning: [1] },
    { index: 155, id: '538e53wsjvmtvefu', meaning: [0] },
    { index: 156, id: '538e51pgk1m7rk0l', meaning: [2, 1, 0] },
    { index: 157, id: '538e51pgk1m7rk18', meaning: [0] },
    { index: 158, id: '538e51pgk1m7rk19', meaning: [0] },
    { index: 159, id: '538e53wsjvmtvf14', meaning: [1, 2] },
    { index: 160, id: '538e51pgk1m7rk1a', meaning: [0] },
    { index: 161, id: '538e51pgk1m7rk17', meaning: [0] },
    { index: 162, id: '538e51pgk1m7rk1b', meaning: [0] },
    { index: 163, id: '538e51pgk1m7rk1c', meaning: [2, 1, 0] },
    { index: 164, id: '538e51pgk1m7rjzx', meaning: [0] },
    { index: 165, id: '538e51pgk1m7rjzy', meaning: [1, 0] },
    { index: 167, id: '538e51pgk1m7rk1d', meaning: [0, 1] },
    { index: 168, id: '538e51pgk1m7rk1e', meaning: [0] },
    { index: 169, id: '538e51pgk1m7rjzz', meaning: [0, 1] },
    { index: 170, id: '538e51pgk1m7rk00', meaning: [0] },
    { index: 171, id: '538e51pgk1m7rk0e', meaning: [0, 1] },
    { index: 172, id: '538e53wsjvmtvf0y', meaning: [0] },
    { index: 174, id: '538e51pgk1m7rk1f', meaning: [0, 1] },
    { index: 175, id: '538e51pgk1m7rk1g', meaning: [0, 1] },
    { index: 176, id: '538e51pgk1m7rk1h', meaning: [0] },
    { index: 177, id: '538e51pgk1m7rk1i', meaning: [0] },
    { index: 178, id: '538e53wsjvmtved0', meaning: [1, 2] },
    { index: 180, id: '538e51pgk1m7rk1j', meaning: [0] },
    { index: 181, id: '538e51pgk1m7rk1k', meaning: [2] },
    { index: 182, id: '538e51pgk1m7rk1l', meaning: [0] },
    { index: 183, id: '538e53wsjvmtveva', meaning: [0, 1] },
    { index: 184, id: '538e51pgk1m7rk1m', meaning: [1, 0] },
    { index: 185, id: '538e51pgk1m7rk1n', meaning: [0] },
    { index: 186, id: '538e51pgk1m7rk1o', meaning: [1, 0] },
    { index: 187, id: '538e51pgk1m7rk1p', meaning: [0] },
    { index: 1000, id: '538e53wsjvmtvedo', meaning: [0] },
    { index: 1001, id: '538e53wsjvmtveg4', meaning: [0] },
    { index: 1003, id: '538e53wsjvmtveg5', meaning: [0] },
    { index: 1004, id: '538e53wsjvmtveg6', meaning: [1] },
    { index: 1005, id: '538e53wsjvmtveg7', meaning: [0] },
    { index: 1006, id: '538e53wsjvmtveg8', meaning: [1, 0] },
    { index: 1007, id: '538e53wsjvmtveg9', meaning: [1, 2] },
    { index: 1008, id: '538e53wsjvmtvega', meaning: [3, 2] },
    { index: 1009, id: '538e53wsjvmtvegb', meaning: [1, 0] },
    { index: 1010, id: '538e53wsjvmtvegc', meaning: [0] },
    { index: 1011, id: '538e53wsjvmtvegd', meaning: [2] },
    { index: 1012, id: '538e54tojtwvi2k3', meaning: [0, 1] },
    { index: 1013, id: '538e53wsjvmtvege', meaning: [0, 1] },
    { index: 1014, id: '538e53wsjvmtvegf', meaning: [0] },
    { index: 1015, id: '538e53wsjvmtvegg', meaning: [0, 1] },
    { index: 1016, id: '538e53wsjvmtvegh', meaning: [0] },
    { index: 1017, id: '538e53wsjvmtvegi', meaning: [0] },
    { index: 1018, id: '538e53wsjvmtvegj', meaning: [0] },
    { index: 1019, id: '538e54j4jtwvjlfx', meaning: [0] },
    { index: 1020, id: '538e53wsjvmtvegk', meaning: [0, 1] }
  ]
}